{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import pymc3 as pm\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/league_train.csv\").sample(frac=1.0)\n",
    "test = pd.read_csv(\"../data/league_test.csv\")\n",
    "\n",
    "f_cols = [col for col in train.columns if col not in [\"matchId\", \"blue_win\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(x):\n",
    "    return [1 if xi >= 0.5 else 0 for xi in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler(feature_range=(0.001, 0.999))\n",
    "X_train = mms.fit_transform(train[f_cols])\n",
    "y_train = train[\"blue_win\"].values\n",
    "X_test = mms.transform(test[f_cols])\n",
    "y_test = test[\"blue_win\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blueGold',\n",
       " 'blueMinionsKilled',\n",
       " 'blueJungleMinionsKilled',\n",
       " 'blueAvgLevel',\n",
       " 'redGold',\n",
       " 'redMinionsKilled',\n",
       " 'redJungleMinionsKilled',\n",
       " 'redAvgLevel',\n",
       " 'blueChampKills',\n",
       " 'blueHeraldKills',\n",
       " 'blueTowersDestroyed',\n",
       " 'redChampKills',\n",
       " 'redHeraldKills',\n",
       " 'redTowersDestroyed']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = np.zeros(len(y_test))\n",
    "preds_test = np.zeros(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seb/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0, penalty=\"none\").fit(X_train, y_train)\n",
    "preds_test = clf.predict_proba(X_test)\n",
    "preds_train = clf.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models AUC score: 0.812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.81       125\n",
      "           1       0.81      0.81      0.81       125\n",
      "\n",
      "    accuracy                           0.81       250\n",
      "   macro avg       0.81      0.81      0.81       250\n",
      "weighted avg       0.81      0.81      0.81       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Models AUC score: {roc_auc_score(y_train, np.argmax(preds_train, axis=1))}\")\n",
    "print(classification_report(y_train, np.argmax(preds_train, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models AUC score: 0.7516653572628913\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.76      0.75     23937\n",
      "           1       0.76      0.75      0.75     24464\n",
      "\n",
      "    accuracy                           0.75     48401\n",
      "   macro avg       0.75      0.75      0.75     48401\n",
      "weighted avg       0.75      0.75      0.75     48401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Models AUC score: {roc_auc_score(y_test, np.argmax(preds_test, axis=1))}\")\n",
    "print(classification_report(y_test, np.argmax(preds_test, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Logistic regression C=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, C=0.3).fit(X_train, y_train)\n",
    "preds_test = clf.predict_proba(X_test)\n",
    "preds_train = clf.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models AUC score: 0.78\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.78       125\n",
      "           1       0.77      0.80      0.78       125\n",
      "\n",
      "    accuracy                           0.78       250\n",
      "   macro avg       0.78      0.78      0.78       250\n",
      "weighted avg       0.78      0.78      0.78       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Models AUC score: {roc_auc_score(y_train, np.argmax(preds_train, axis=1))}\")\n",
    "print(classification_report(y_train, np.argmax(preds_train, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models AUC score: 0.7817045293341828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     23937\n",
      "           1       0.78      0.78      0.78     24464\n",
      "\n",
      "    accuracy                           0.78     48401\n",
      "   macro avg       0.78      0.78      0.78     48401\n",
      "weighted avg       0.78      0.78      0.78     48401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Models AUC score: {roc_auc_score(y_test, np.argmax(preds_test, axis=1))}\")\n",
    "print(classification_report(y_test, np.argmax(preds_test, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Alpha is the interception\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sd=10)\n",
    "    # The prior for the features varibles which are included\n",
    "    #beta = pm.Normal(\"beta\", mu=0, sd=3, shape=X.shape[1])\n",
    "    b_gold = pm.Beta(\"b_gold\", alpha=3, beta=3, observed=X_train[:,f_cols.index(\"blueGold\")])\n",
    "    b_minion = pm.Beta(\"b_minion\", alpha=3, beta=3, observed=X_train[:,f_cols.index(\"blueMinionsKilled\")])\n",
    "    b_jungle_camps = pm.Beta(\"b_camps\", alpha=3, beta=3, observed=X_train[:,f_cols.index(\"blueJungleMinionsKilled\")])\n",
    "    b_avg_level = pm.Beta(\"b_level\", alpha=3, beta=3, observed=X_train[:,f_cols.index(\"blueAvgLevel\")])\n",
    "    b_champ_kills = pm.Beta(\"b_kills\", alpha=3, beta=3, observed=X_train[:,f_cols.index(\"blueChampKills\")])\n",
    "    b_towers_destroyed = pm.Beta(\"b_towers\", alpha=3, beta=5, observed=X_train[:,f_cols.index(\"blueTowersDestroyed\")])\n",
    "    b_herald_kills = pm.Beta(\"b_heralds\", alpha=3, beta=5, observed=X_train[:,f_cols.index(\"blueHeraldKills\")])\n",
    "    \n",
    "    r_gold = pm.Beta(\"r_gold\", alpha=3, beta=3, observed=X_train[:,f_cols.index(\"redGold\")])\n",
    "    r_minion = pm.Beta(\"r_minion\", alpha=3, beta=3, observed=X_train[:,f_cols.index(\"redMinionsKilled\")])\n",
    "    r_jungle_camps = pm.Beta(\"r_camps\", alpha=3, beta=3, observed=X_train[:,f_cols.index(\"redJungleMinionsKilled\")])\n",
    "    r_avg_level = pm.Beta(\"r_level\", alpha=3, beta=3, observed=X_train[:,f_cols.index(\"redAvgLevel\")])\n",
    "    r_champ_kills = pm.Beta(\"r_kills\", alpha=3, beta=3, observed=X_train[:,f_cols.index(\"redChampKills\")])\n",
    "    r_towers_destroyed = pm.Beta(\"r_towers\", alpha=3, beta=5, observed=X_train[:,f_cols.index(\"redTowersDestroyed\")])\n",
    "    r_herald_kills = pm.Beta(\"r_heralds\", alpha=3, beta=5, observed=X_train[:,f_cols.index(\"redHeraldKills\")])\n",
    "    \n",
    "    #beta = pm.Normal(\"beta\", mu=0, sd=10, shape=X_train.shape[1])\n",
    "    beta_b_gold = pm.Normal(\"beta_b_gold\", mu=6, sd=3)\n",
    "    beta_b_minion = pm.Normal(\"beta_b_minion\", mu=2, sd=1)\n",
    "    beta_b_jungle_camps = pm.Normal(\"beta_b_camps\", mu=2, sd=1)\n",
    "    beta_b_avg_level = pm.Normal(\"beta_b_level\", mu=2, sd=1)\n",
    "    beta_b_champ_kills = pm.Normal(\"beta_b_kills\", mu=2, sd=1)\n",
    "    beta_b_towers_destroyed = pm.Normal(\"beta_b_towers\", mu=2, sd=1)\n",
    "    beta_b_herald_kills = pm.Normal(\"beta_b_heralds\", mu=2, sd=1)\n",
    "\n",
    "\n",
    "    beta_r_gold = pm.Normal(\"beta_r_gold\", mu=-6, sd=3)\n",
    "    beta_r_minion = pm.Normal(\"beta_r_minion\", mu=-2, sd=1)\n",
    "    beta_r_jungle_camps = pm.Normal(\"beta_r_camps\", mu=-2, sd=1)\n",
    "    beta_r_avg_level = pm.Normal(\"beta_r_level\", mu=-2, sd=1)\n",
    "    beta_r_champ_kills = pm.Normal(\"beta_r_kills\", mu=-2, sd=1)\n",
    "    beta_r_towers_destroyed = pm.Normal(\"beta_r_towers\", mu=-2, sd=1)\n",
    "    beta_r_herald_kills = pm.Normal(\"beta_r_heralds\", mu=-2, sd=1)\n",
    "\n",
    "    # Deterministic function\n",
    "    #p = pm.math.dot(X,beta)\n",
    "    p = (b_gold * beta_b_gold +\n",
    "         b_minion * beta_b_minion +\n",
    "         b_jungle_camps * beta_b_jungle_camps +\n",
    "         b_avg_level * beta_b_avg_level +\n",
    "         b_champ_kills * beta_b_champ_kills +\n",
    "         b_towers_destroyed * beta_b_towers_destroyed +\n",
    "         b_herald_kills * beta_b_herald_kills +\n",
    "         r_gold * beta_r_gold +\n",
    "         r_minion * beta_r_minion +\n",
    "         r_jungle_camps * beta_r_jungle_camps +\n",
    "         r_avg_level * beta_r_avg_level +\n",
    "         r_champ_kills * beta_r_champ_kills +\n",
    "         r_towers_destroyed * beta_r_towers_destroyed +\n",
    "         r_herald_kills * beta_r_herald_kills\n",
    "        )\n",
    "\n",
    "    y_obs = pm.Bernoulli(\"y_obs\", pm.invlogit(p + alpha), observed=y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-3b2e2b36db61>:2: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  trace = pm.sample(tune=2000, draws = 2000, random_seed = 0, cores = 1, progressbar = True, chains = 1)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [beta_r_heralds, beta_r_towers, beta_r_kills, beta_r_level, beta_r_camps, beta_r_minion, beta_r_gold, beta_b_heralds, beta_b_towers, beta_b_kills, beta_b_level, beta_b_camps, beta_b_minion, beta_b_gold, alpha]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4000' class='' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [4000/4000 00:26<00:00 Sampling chain 0, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 1 chain for 2_000 tune and 2_000 draw iterations (2_000 + 2_000 draws total) took 27 seconds.\n"
     ]
    }
   ],
   "source": [
    "with model:\n",
    "    trace = pm.sample(tune=2000, draws = 2000, random_seed = 0, cores = 1, progressbar = True, chains = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'beta_b_gold':np.apply_along_axis(np.mean, 0, trace['beta_b_gold']),\n",
    "                        'beta_b_minion':np.apply_along_axis(np.mean, 0, trace['beta_b_minion']),\n",
    "                        'beta_b_camps':np.apply_along_axis(np.mean, 0, trace['beta_b_camps']),\n",
    "                        'beta_b_level':np.apply_along_axis(np.mean, 0, trace['beta_b_level']),\n",
    "                        'beta_b_kills':np.apply_along_axis(np.mean, 0, trace['beta_b_kills']),\n",
    "                        'beta_b_towers':np.apply_along_axis(np.mean, 0, trace['beta_b_towers']),\n",
    "                        'beta_b_heralds': np.apply_along_axis(np.mean, 0, trace['beta_b_heralds']),\n",
    "                        'beta_r_gold':np.apply_along_axis(np.mean, 0, trace['beta_r_gold']),\n",
    "                        'beta_r_minion':np.apply_along_axis(np.mean, 0, trace['beta_r_minion']),\n",
    "                        'beta_r_camps':np.apply_along_axis(np.mean, 0, trace['beta_r_camps']),\n",
    "                        'beta_r_level':np.apply_along_axis(np.mean, 0, trace['beta_r_level']),\n",
    "                        'beta_r_kills':np.apply_along_axis(np.mean, 0, trace['beta_r_kills']),\n",
    "                        'beta_r_towers':np.apply_along_axis(np.mean, 0, trace['beta_r_towers']),\n",
    "                        'beta_r_heralds': np.apply_along_axis(np.mean, 0, trace['beta_r_heralds']),\n",
    "                        'alpha':np.apply_along_axis(np.mean, 0, trace['alpha'])\n",
    "                       }, index=[0])\n",
    "results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = np.apply_along_axis(np.mean, 1, expit(trace['alpha'] + np.dot(X_train, np.transpose(results.drop(\"alpha\", axis=1)) )) )\n",
    "print(f\"Models AUC score: {roc_auc_score(y_train, preds_train)}\")\n",
    "print(classification_report(y_train, get_predictions(preds_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = np.apply_along_axis(np.mean, 1, expit(trace['alpha'] + np.dot(X_test, np.transpose(results.drop(\"alpha\", axis=1)) )) )\n",
    "print(f\"Models AUC score: {roc_auc_score(y_test, preds_test)}\")\n",
    "print(classification_report(y_test, get_predictions(preds_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_estimate = pm.find_MAP(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = np.apply_along_axis(np.mean, 1, expit(trace['alpha'] + np.dot(X_train, np.transpose(pd.DataFrame(map_estimate, index=[0]).drop(\"alpha\", axis=1)) )) )\n",
    "print(f\"Models AUC score: {roc_auc_score(y_train, preds_train)}\")\n",
    "print(classification_report(y_train, get_predictions(preds_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = np.apply_along_axis(np.mean, 1, expit(trace['alpha'] + np.dot(X_test, np.transpose(pd.DataFrame(map_estimate, index=[0]).drop(\"alpha\", axis=1)) )) )\n",
    "print(f\"Models AUC score: {roc_auc_score(y_test, preds_test)}\")\n",
    "print(classification_report(y_test, get_predictions(preds_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = gnb.predict(X_train)\n",
    "print(f\"Models AUC score: {roc_auc_score(y_train, preds_train)}\")\n",
    "print(classification_report(y_train, get_predictions(preds_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = gnb.predict(X_test)\n",
    "print(f\"Models AUC score: {roc_auc_score(y_test, preds_test)}\")\n",
    "print(classification_report(y_test, get_predictions(preds_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
